[<img width=900 src="../img/title.png?raw=yes">](../README.md)   
[Syllabus](../README.md) |
[Slides and Assignments](README.md) |
[Project](project.md) |
[Lecturer](http://zhe-yu.github.io) 

For PA1-4 we will be working with two new datasets, iris.txt and agaricus-lepiota.txt from the [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml/index.php).

You will need to implement the following:

1. (1 point) Data normalization - there are three methods in DataSet.java, getInputMeans, getInputStandardDeviations and normalize, you will need to implement all three.

2. (1 point) the SVM loss function in NeuralNetwork.java

3. (1 point) the SOFTMAX loss function in NeuralNetwork.java

4. (1 point) calculateAccuracy in NeuralNetwork.java - this will calculate the total number of correct classifications the network has made and report it as a percentage. Note that for both the SVM and SOFTMAX layers, the output node with the highest output will be the predicted class.

5. (1 point) Nesterov momentum in PA14GradientDescent.java

6. (2 points) Create a data/ConvertMushroom.java class that will take the raw mushroom data in agaricus-lepiota.data and convert it to the one-hot encoding found in agaricus-lepiota.txt (you can use my provided data set to make sure you have implemented this correctly).

There is also a few opportunities for bonus:

1. (5%) Implement RMSprop

2. (5%) Implement Adam 

For this assignment there are two test files:

PA14Tests.java - This takes one command line argument which can be either "svm" or "softmax" which will test that you've implemented the SVM and SOFMAX loss functions correctly.

PA14GradientDescent.java - This will actually run gradient descent on a specific network of a given type with provided hyperparameters. In it, you will need to implement the stochasticEpoch, minibatchEpoch, and batchEpoch methods, each of which will perform a single epoch of the specified type of gradient descent.  This will print the error and accuracy of the network over the whole dataset after each epoch so you can see how well your code is training. You can run this as follows:

java PA14GradientDescent <data set=""> <gradient descent="" type=""> <batch size=""> <loss function=""> <epochs> <bias> <learning rate=""> <mu> <layer_size_1 ...="" layer_size_n="">

Where:

data set can be: 'and', 'or', 'xor', 'iris' or 'mushroom'

gradient descent type can be: 'stochastic', 'minibatch' or 'batch'

batch size should be &gt; 0. Will be ignored for stochastic or batch gradient descent.

loss function can be: 'l1_norm', 'l2 norm', 'svm' or 'softmax'. note that you need to use l1 or l2 norm for the or, and or xor data, and svm or softmax for the iris or mushroom data

epochs is an integer &gt; 0

bias is a double, start with 0.1

learning rate is a double usually small and &gt; 0, I suggest starting with 0.3

mu is a double &lt; 1 and typical values are 0.5, 0.9, 0.95 and 0.99

After this you can specify the layer sizes.

So for example:

java PA14GradientDescent mushroom minibatch 20 softmax 100 0.1 0.01 0.9 10 10

Will run gradient descent on the mushroom data set with minibatch gradient with a batch size of 20 for 100 epochs, initializing node bias to 0.1, using a learning rate of 0.01, a mu of 0.9, and a network with two layers each with 10 nodes.

Programming Assignment 1 is 15% of your final grade, and PA1-4 is 20% of that. It will be traded out of 7 points as described above. You also have the potential for %10 extra points of bonus.</layer_size_1></mu></learning></bias></epochs></loss></batch></gradient></data>
