[<img width=900 src="../img/title.png?raw=yes">](../README.md)   
[Syllabus](../README.md) |
[Slides and Assignments](README.md) |
[Project](project.md) |
[Lecturer](http://zhe-yu.github.io) 

For PA1-3, we will be working with two new datasets, and.txt and or.txt, so you have some additional data sets to play with.

For NeuralNetwork.java the constructor now takes a LossFunction value from the LossFunction.java enum. This can potentially be NONE (like we've been doing up until now), L1_NORM, L2_NORM, SVM or SOFTMAX. You will need to implement the L1 and L2 norm loss functions for this assignment.

ActivationType.NONE is now ActivationType.LINEAR and you can implement applyLinear() in Node.java if you'd like.

In NeuralNetwork.forwardPass(Instance instance) you will need to implement the L1_NORM and L2_NORM loss functions. Some wrapper code and TODOs show where you should do this. Make sure you update the postActivationValues for the output node(s) after the loss function, and set their delta values to the appropriate gradient (the gradients are from the lecture notes).

Additionally, forwardPass will now return the output of the loss function (which you can use to simplify your numeric gradient calculation code if you'd like). I have added the code for the previous cases where we had no loss function (so the return value is simply the sum of all the outputs), you need to update this for the L1 and L2 norm loss functions.

You will also need to implement the NeuralNetwork.initializeRandomly(double bias) method to properly initialize the weights and biases (as per the lecture notes) using a normal (gaussian) distribution, where the random input weights for each node are divided by the fan-in value of that node. You can use the nextGaussian() method of Java's Random class to generate a random value from a normal (gaussian) distribution with 0 mean and 1 standard deviation.  You need to implement the Node.initializeWeightsAndBias(double bias) method as a helper.

For this assignment there are two test files:

PA13Tests.java - This takes one command line argument which can be either "l1_norm" or "l2_norm" which will test that you've implemented the L1 and L2 norms correctly.

PA13GradientDescent.java - This will actually run gradient descent on a specific network of a given type with provided hyperparameters. In it, you will need to implement the stochasticEpoch, minibatchEpoch, and batchEpoch methods, each of which will perform a single epoch of the specified type of gradient descent.  This will print the error of the network over the whole dataset after each epoch so you can see how well your code is training. You can run this as follows:

java PA13GradientDescent <data set=""> <network type=""> <gradient descent="" type=""> <loss function=""> <epochs> <bias> <learning rate="">

Where:

data set can be: 'and', 'or' or 'xor'

network type can be: 'tiny', 'small' or 'large'

gradient descent type can be: 'stochastic', 'minibatch' or 'batch'

loss function can be: 'l1_norm', or 'l2 norm'

epochs is an integer &gt; 0

bias is a double

learning rate is a double usually small and &gt; 0, I suggest starting with 0.3

Programming Assignment 1 is 15% of your final grade, and PA1-3 is 20% of that. It will be traded out of 6 points as follows:

(1 point) L1_NORM loss function implemented correctly.

(1 point) L2_NORM loss function implemented correctly.

(1 point) initializeRandomly implemented correctly.

(1 point each) implementing stochastic, minibatch and batch gradient descent epochs correctly.

NOTE: For each of the datasets, even if you've implemented everything correctly, you may notice that your error will start around 2 and get stuck around 1.0. Sometimes however it will get down to 0. This is because these search landscapes have some flat areas which the network can get stuck on around 1.0. So this is not a bug with your implementation. When we add momentum in PA1-4 we should be able to break free from these regions better.</learning></bias></epochs></loss></gradient></network></data>
