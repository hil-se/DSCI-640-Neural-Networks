[<img width=900 src="../img/title.png?raw=yes">](../README.md)   
[Syllabus](../README.md) |
[Slides and Assignments](README.md) |
[Project](project.md) |
[Lecturer](http://zhe-yu.github.io) 

The focus of this assignment is to implement dropout and batch normalization.

Programming Assignment 3 is 15% of your final grade, and Part 2 is 33% of that. It will be graded out of 6 points, with 6 potential bonus points.

Note: For the LeNet-5 architecture, the tests were generated with the last hidden layer not using dropout, so please turn it off in your implementation for that layer.

You need to implement the following:

1. (1 points) Update propagateForward and propagateBackward to perform dropout (use inverted dropout). Please note that you will need to use generator.nextRandom() to generate random values for dropout -- this is required so that the ConvolutionalNode can be used to calculate a numeric gradient to compare to the backprop gradient.

2. (3 point) Update propagateForward and propagateBackward to perform batch normalization.

3. (2 points) Use GradientDescent.java to find a set of hyperparameters which can achieve 99.0% or higher accuracy on the MNIST test data. Include in your submission a mnist_99.txt log file which contains the output of your gradient descent run (and the hyperparameters you used to achieve it).

5. (BONUS 5%) Use GradientDescent.java to find a set of hyperparameters which can achieve 99.3% or higher accuracy on the MNIST test data. Include in your submission a mnist_99.3.txt log file which contains the output of your gradient descent run (and the hyperparameters you used to achieve it).

5. (BONUS 10%) Use GradientDescent.java to find a set of hyperparameters which can achieve 99.5% or higher accuracy on the MNIST test data. Include in your submission a mnist_99.5.txt log file which contains the output of your gradient descent run (and the hyperparameters you used to achieve it). You will probably need to implement ADAM or RMSPROP or similar to achieve this (include your settings for that as well).

You can verify that your code is working by using PA33Tests.java and PA33TestCNNs.java, which will test dropout, batch normalization and then full neural networks.
