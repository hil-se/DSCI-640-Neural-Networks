[<img width=900 src="../img/title.png?raw=yes">](../README.md)   
[Syllabus](../README.md) |
[Slides and Assignments](README.md) |
[Project](project.md) |
[Lecturer](http://zhe-yu.github.io) 

The focus of this assignment is to implement the backward pass through a CNN and gradient descent.

Programming Assignment 3 is 15% of your final grade, and Part 2 is 34% of that. It will be graded out of 7 points, with 2 potential bonus points.




You need to implement the following:

1. (3 points) Complete backwardPass in ConvolutionalNeuralNetwork, and propagateBackward in ConvolutionalNode, ConvolutionalEdge and Pooling Edge.

2. (1 point) Complete calculateAccuracyAndError in ConvolutionalNeuralNetwork.

3. (1 point) Complete GradientDescent.java - you'll need to implement minibatch gradient descent with Nesterov momentum.

4. (2 points) Use GradientDescent.java to find a set of hyperparameters which can achieve 98% or higher accuracy on the MNIST test data. Include in your submission a mnist_98.txt log file which contains the output of your gradient descent run (and the hyperparameters you used to achieve it).

5. (BONUS 10%) Use GradientDescent.java to find a set of hyperparameters which can achieve 99% or higher accuracy on the MNIST test data. Include in your submission a mnist_99.txt log file which contains the output of your gradient descent run (and the hyperparameters you used to achieve it).




You can verify that your backward pass code is working by using PA32Tests.java and PA32TestCNNs.java.
